--Q.Will the reducer work or not if you use “Limit 1” in any HiveQL query?
ANS- Yes, using "LIMIT 1" in a HiveQL query can affect the reducer's functionality. In Hive, the "LIMIT" clause is used to restrict the number of rows returned by a query. When you use "LIMIT 1", it instructs Hive to return only the first row that satisfies the query conditions.
The reducer is a phase in the MapReduce framework responsible for reducing the intermediate key-value pairs generated by the mapper into a smaller set of output key-value pairs. 
The reducer operates on a subset of data, typically grouped by keys.

Q.Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
Ans-  No, it won't allow.

Q.Suppose, I create a table that contains details of all the transactions done by the customers: 
CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. 
But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans- To improve the performance of your query and calculate the total revenue generated for each month more efficiently, you can take the following steps:
Partitioning: Partitioning the table based on the "month" column can significantly enhance query performance. Partitioning involves dividing the data into logical partitions
based on a specific column. In this case, you can create a partitioned table where each partition represents a specific month.

Q.How can you add a new partition for the month December in the above partitioned table?
Ans- Alter the table to add a new partition: Use the ALTER TABLE statement in Hive to add a new partition to the partitioned table.

ALTER TABLE transaction_details ADD PARTITION (month='December');


Q.I am inserting data into a table based on partitions dynamically.
But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
Ans- Have to set this parameter to remove the error:

SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

Q.Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
Ans-- First we have to create a table with Serde properties and then simply load the data.

CREATE TABLE my_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  "separatorChar" = ",",
  "quoteChar" = "\"",
  "escapeChar" = "\\"
)
STORED AS TEXTFILE;

For loading: LOAD DATA INPATH '/temp/sample.csv' INTO TABLE my_table;

Q.Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Ans-Create an external Hive table: Create an external Hive table that points to the directory where the consolidated SequenceFile(s) are stored.
The external table definition should match the structure of the data in the SequenceFiles. For example:

CREATE EXTERNAL TABLE my_table (
  id INT,
  name STRING,
  email STRING,
  country STRING
)
STORED AS SEQUENCEFILE
LOCATION '/path/to/sequencefiles';


Q.LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
Ans- File not present in the location.

Q.Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans- Yes,it is possible.
To add 100 nodes to an existing Hive cluster, you need to provision new nodes with the required hardware or virtual machines, 
install and configure Hadoop on these nodes, update the cluster's configuration files to include the new nodes, and restart the Hadoop services on the existing 
nodes to recognize the new nodes. This process expands the underlying Hadoop infrastructure, which Hive leverages for distributed processing, allowing it to utilize
the additional resources provided by the new nodes.



1. Create a hive table as per given schema in your dataset 
Create table sales_order_csv(
ORDERNUMBER  int,
QUANTITYORDERED double,
PRICEEACH int,
ORDERLINENUMBER int,
SALES float,
STATUS varchar(50),
QTR_ID int,
MONTH_ID int,
YEAR_ID int,
PRODUCTLINE varchar(50),
MSRP int,
PRODUCTCODE varchar(50),
PHONE bigint,
CITY varchar(60),
STATE varchar(50),
POSTALCODE int,
COUNTRY varchar(50),
TERRITORY varchar(50),
CONTACTLASTNAME varchar(50),
CONTACTFIRSTNAME varchar(50),
DEALSIZE varchar(60))
row format delimited fields terminated by ','
tblproperties ("skip.header.line.count" = "1");

2. try to place a data into table location
load data inpath '/myfiles/sales_order_csv.csv' into table sales_order_csv;

3. Perform a select operation .
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH from sales_order_csv;

4. Fetch the result of the select operation in your local as a csv file .
CREATE TABLE temp_table AS
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH from sales_order_csv;

INSERT OVERWRITE DIRECTORY '/tmp/result'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
SELECT ORDERNUMBER,QUANTITYORDERED,PRICEEACH
FROM temp_table;

hadoop fs -copyToLocal /tmp/result/000000_0 result.csv


5. Perform group by operation . 
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv GROUP BY STATUS;

7. Perform filter operation at least 5 kinds of filter examples . 
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS ='sHIPPED';

Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE QUANTITYORDERED>2;

Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS LIKE 'SHI%';

Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE QUANTITYORDERED BETWEEN 2 AND 5;

Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS IN ('SHIPPED','IN-PROCESS');


8. show and example of regex operation

9. alter table operation
ALTER TABLE sales_order_csv
DROP COLUMN DEAL_SIZE;

10 . drop table operation
ALTER TABLE sales_order_csv
DROP COLUMN DEAL_SIZE;

12 . order by operation . 
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS ='sHIPPED' ORDER BY QUANTITYORDERED;

13 . where clause operations you have to perform . 
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS LIKE 'SHI%';

14 . sorting operation you have to perform . 
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS ='sHIPPED' ORDER BY QUANTITYORDERED ASC;

15 . distinct operation you have to perform . 
Select  DISTINCT ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS ='sHIPPED' ORDER BY QUANTITYORDERED;

16 . like an operation you have to perform . 
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS LIKE 'SHI%';

17 . union operation you have to perform . 

As there is only one table.
 For example :
SELECT employee_name
FROM employees1
UNION
SELECT employee_name
FROM employees2;



18 . table view operation you have to perform . 

CREATE VIEW shipped_table AS
Select ORDERNUMBER,QUANTITYORDERED,PRICEEACH,STATUS from sales_order_csv WHERE STATUS ='SHIPPED';





#hive operation with python

##Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.
fetch rows to python itself into a list of tuples and mimic the join or filter operations

from pyhive import hive

# Establish connection to Hive
conn = hive.Connection(
    host='your_hive_host',
    port=your_hive_port,
    username='your_username'
)

# Create a cursor object
cursor = conn.cursor()

# Perform filter operation
cursor.execute("SELECT * FROM your_table WHERE column1 = 'value'")
filtered_data = cursor.fetchall()

# Perform regex operation
cursor.execute("SELECT * FROM your_table WHERE column2 RLIKE '.+pattern'")
regex_matched_data = cursor.fetchall()

# Perform ALTER TABLE operation
cursor.execute("ALTER TABLE your_table ADD COLUMN new_column STRING")

# Perform DROP TABLE operation
cursor.execute("DROP TABLE your_table")

# Perform ORDER BY operation
cursor.execute("SELECT * FROM your_table ORDER BY column1 DESC")
ordered_data = cursor.fetchall()

# Perform WHERE clause operation
cursor.execute("SELECT * FROM your_table WHERE column1 = 'value' AND column2 > 10")
where_filtered_data = cursor.fetchall()

# Perform DISTINCT operation
cursor.execute("SELECT DISTINCT column1 FROM your_table")
distinct_values = cursor.fetchall()

# Perform LIKE operation
cursor.execute("SELECT * FROM your_table WHERE column1 LIKE 'prefix%'")
like_matched_data = cursor.fetchall()

# Perform UNION operation
cursor.execute("SELECT column1 FROM table1 UNION SELECT column1 FROM table2")
union_data = cursor.fetchall()

# Perform CREATE VIEW operation
cursor.execute("CREATE VIEW your_view AS SELECT column1, column2 FROM your_table")

# Fetch rows into a list of tuples
data_list = []
for row in cursor.fetchall():
    data_list.append(row)

# Close the connection
conn.close()

# Print the fetched data
for row in data_list:
    print(row)

