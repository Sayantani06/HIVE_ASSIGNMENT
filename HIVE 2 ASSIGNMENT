Q.Will the reducer work or not if you use “Limit 1” in any HiveQL query?
ANS- Yes, using "LIMIT 1" in a HiveQL query can affect the reducer's functionality. In Hive, the "LIMIT" clause is used to restrict the number of rows returned by a query. When you use "LIMIT 1", it instructs Hive to return only the first row that satisfies the query conditions.
The reducer is a phase in the MapReduce framework responsible for reducing the intermediate key-value pairs generated by the mapper into a smaller set of output key-value pairs. 
The reducer operates on a subset of data, typically grouped by keys.

Q.Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
Ans-  No, it won't allow.

Q.Suppose, I create a table that contains details of all the transactions done by the customers: 
CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. 
But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans- To improve the performance of your query and calculate the total revenue generated for each month more efficiently, you can take the following steps:
Partitioning: Partitioning the table based on the "month" column can significantly enhance query performance. Partitioning involves dividing the data into logical partitions
based on a specific column. In this case, you can create a partitioned table where each partition represents a specific month.

Q.How can you add a new partition for the month December in the above partitioned table?
Ans- Alter the table to add a new partition: Use the ALTER TABLE statement in Hive to add a new partition to the partitioned table.

ALTER TABLE transaction_details ADD PARTITION (month='December');


Q.I am inserting data into a table based on partitions dynamically.
But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
Ans- Have to set this parameter to remove the error:

SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

Q.Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
Ans-- First we have to create a table with Serde properties and then simply load the data.

CREATE TABLE my_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  "separatorChar" = ",",
  "quoteChar" = "\"",
  "escapeChar" = "\\"
)
STORED AS TEXTFILE;

For loading: LOAD DATA INPATH '/temp/sample.csv' INTO TABLE my_table;

Q.Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Ans-Create an external Hive table: Create an external Hive table that points to the directory where the consolidated SequenceFile(s) are stored.
The external table definition should match the structure of the data in the SequenceFiles. For example:

CREATE EXTERNAL TABLE my_table (
  id INT,
  name STRING,
  email STRING,
  country STRING
)
STORED AS SEQUENCEFILE
LOCATION '/path/to/sequencefiles';


Q.LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
Ans- File not present in the location.

Q.Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans- Yes,it is possible.
To add 100 nodes to an existing Hive cluster, you need to provision new nodes with the required hardware or virtual machines, 
install and configure Hadoop on these nodes, update the cluster's configuration files to include the new nodes, and restart the Hadoop services on the existing 
nodes to recognize the new nodes. This process expands the underlying Hadoop infrastructure, which Hive leverages for distributed processing, allowing it to utilize
the additional resources provided by the new nodes.



